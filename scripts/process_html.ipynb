{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "import multiprocess as mp\n",
    "\n",
    "\n",
    "main_dir = \"/Users/cn/Desktop/Dissertation/mandm/data/articles/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_csv(file_path):\n",
    "    file = open(file_path, 'r')\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "    title_html = soup.find(\"meta\", property = \"og:title\")\n",
    "    title = title_html[\"content\"]\n",
    "    site_name_html = soup.find(\"meta\", property=\"og:site_name\")\n",
    "    site_name = site_name_html[\"content\"]\n",
    "    date_html = soup.find(\"meta\", property = \"article:published_time\")\n",
    "    date = date_html[\"content\"][0:10]\n",
    "    paragraphs = soup.findAll(\"p\")\n",
    "    articles = ''\n",
    "    for paragraph in paragraphs:\n",
    "        articles += '{} '.format(paragraph)\n",
    "    row = [title, date, articles, site_name]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "headers = [\"title\", \"date\", \"article\", \"site_name\"]\n",
    "root_path = \"/Users/cn/Desktop/Dissertation/mandm/data/articles/al_dia/\"\n",
    "paths = os.listdir(root_path)\n",
    "\n",
    "\n",
    "with open(\"/Users/cn/Desktop/Dissertation/mandm/data/al_dia.csv\", 'w', newline='') as csvfile:\n",
    "    csv_file = csv.writer(csvfile, dialect='excel')\n",
    "    csv_file.writerow(headers)\n",
    "    for path in paths:\n",
    "        to_file = root_path + path\n",
    "        #print(to_file)\n",
    "        try:\n",
    "            row = html_to_csv(to_file, \"body\")\n",
    "        #print(row)\n",
    "            csv_file.writerow(row)\n",
    "        except Exception:\n",
    "            pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "root_path = \"/Users/cn/Desktop/Dissertation/mandm/data/articles/diario_del_césar/\"\n",
    "paths = os.listdir(root_path)\n",
    "\n",
    "\n",
    "with open(\"/Users/cn/Desktop/Dissertation/mandm/data/diario_del_césar.csv\", 'w', newline='') as csvfile:\n",
    "    csv_file = csv.writer(csvfile, dialect='excel')\n",
    "    csv_file.writerow(headers)\n",
    "    for path in paths:\n",
    "        to_file = root_path + path\n",
    "        #print(to_file)\n",
    "        try:\n",
    "            row = html_to_csv(to_file)\n",
    "        #print(row)\n",
    "            csv_file.writerow(row)\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sites(root_path, paths, site):\n",
    "    csv_name = \"/Users/cn/Desktop/Dissertation/mandm/data/\" + site + \".csv\"\n",
    "    with open(csv_name, 'w', newline='') as csvfile:\n",
    "        csv_file = csv.writer(csvfile, dialect='excel')\n",
    "        csv_file.writerow(headers)\n",
    "        for path in paths:\n",
    "            to_file = root_path + path\n",
    "            #print(to_file)\n",
    "            try:\n",
    "                row = html_to_csv(to_file)\n",
    "            #print(row)\n",
    "                csv_file.writerow(row)\n",
    "            except Exception:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "site = \"diario_del_norte\"\n",
    "root_path = main_dir + site + \"/\"\n",
    "paths = os.listdir(root_path)\n",
    "\n",
    "convert_sites(root_path, paths, site)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "site = \"diario_del_sur\"\n",
    "root_path = main_dir + site + \"/\"\n",
    "paths = os.listdir(root_path)\n",
    "\n",
    "convert_sites(root_path, paths, site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"title\", \"date\", \"article\", \"site_name\"]\n",
    "all_sites = os.listdir(main_dir)\n",
    "all_sites.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_all(all_sites):\n",
    "    for site in all_sites:\n",
    "        try:\n",
    "            root_path = main_dir + site + \"/\"\n",
    "            paths = os.listdir(root_path)\n",
    "            convert_sites(root_path, paths, site)\n",
    "            print(site)\n",
    "        except Exception:\n",
    "            print(\"Error: \" + site + \" didn't work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el_diario\n",
      "el_heraldo\n",
      "vanguardia_liberal\n",
      "vanguardia_liberal_2\n",
      "el_país\n",
      "hoy_diario_de_magdalena_2\n",
      "el_meridiano\n",
      "el_nuevo_liberal\n",
      "elcolombiano\n",
      "el_manduco\n",
      "diario_del_norte\n",
      "la_opinion\n",
      "el_expreso\n",
      "la_nación\n",
      "periodismo_público\n",
      "la_crónica_del_quíndio\n",
      "eje21\n",
      "el_nuevo_dia\n",
      "llano_siete_dias\n",
      "diario_del_césar\n",
      "el_universal_2\n",
      "el_universal\n",
      "the_archipielago_press\n",
      "al_dia\n",
      "diario_del_sur\n",
      "hoy_diario_de_magdalena\n",
      "elpais\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[None]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = mp.Pool(mp.cpu_count()-2)\n",
    "[pool.map(convert_all, [all_sites])]\n",
    "#convert_all(['el_diario'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sites that didn't work with the default functions above:\n",
    "- el pais\n",
    "- diario del cesar\n",
    "- el nuevo dia\n",
    "- la cronica del quindio\n",
    "- el expreso \n",
    "- la opinion\n",
    "- el heraldo\n",
    "- el meridioano\n",
    "- el nuevo dia\n",
    "- el nuevo liberal\n",
    "- elpais\n",
    "- vanguardia liberal \n",
    "- vanguardia liberal2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'el_país'\n",
    "root_path = main_dir + site + \"/\"\n",
    "paths = os.listdir(root_path)\n",
    "convert_sites2(root_path, paths, site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_csv2(file_path):\n",
    "    file = open(file_path, 'r')\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "    title_html = soup.find(\"meta\", property = \"spc:title\")\n",
    "    title = title_html[\"content\"]\n",
    "    site_name_html = soup.find(\"meta\", property=\"og:site_name\")\n",
    "    site_name = site_name_html[\"content\"]\n",
    "    date_html = soup.find(\"meta\", property = \"spc:published_time\")\n",
    "    date = date_html[\"content\"][0:10]\n",
    "    paragraphs = soup.findAll(\"p\")\n",
    "    articles = ''\n",
    "    for paragraph in paragraphs:\n",
    "        articles += '{} '.format(paragraph)\n",
    "    row = [title, date, articles, site_name]\n",
    "    return row\n",
    "\n",
    "def convert_sites2(root_path, paths, site):\n",
    "    csv_name = \"/Users/cn/Desktop/Dissertation/mandm/data/\" + site + \".csv\"\n",
    "    with open(csv_name, 'w', newline='') as csvfile:\n",
    "        csv_file = csv.writer(csvfile, dialect='excel')\n",
    "        csv_file.writerow(headers)\n",
    "        for path in paths:\n",
    "            to_file = root_path + path\n",
    "            #print(to_file)\n",
    "            try:\n",
    "                row = html_to_csv2(to_file)\n",
    "            #print(row)\n",
    "                csv_file.writerow(row)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "def convert_all2(all_sites):\n",
    "    for site in all_sites:\n",
    "        try:\n",
    "            root_path = main_dir + site + \"/\"\n",
    "            paths = os.listdir(root_path)\n",
    "            convert_sites2(root_path, paths, site)\n",
    "            print(site)\n",
    "        except Exception:\n",
    "            print(\"Error: \" + site + \" didn't work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"title\", \"date\", \"article\", \"site_name\"]\n",
    "all_sites2 = ['el_heraldo', 'vanguardia_liberal', 'vanguardia_liberal_2',\n",
    "    'el_país', 'el_meridiano', 'el_nuevo_liberal', 'la_opinion', 'el_expreso',\n",
    "    'el_nuevo_dia', 'diario_del_césar', 'la_crónica_del_quíndio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el_heraldo\n",
      "vanguardia_liberal\n",
      "vanguardia_liberal_2\n",
      "el_país\n",
      "el_meridiano\n",
      "el_nuevo_liberal\n",
      "la_opinion\n",
      "el_expreso\n",
      "el_nuevo_dia\n",
      "diario_del_césar\n",
      "la_crónica_del_quíndio\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[None]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = mp.Pool(mp.cpu_count()-2)\n",
    "[pool.map(convert_all, [all_sites2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_csv3(file_path, title_property, site_name_property, date_property):\n",
    "    file = open(file_path, 'r')\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "    title_html = soup.find(\"meta\", property = title_property)\n",
    "    title = title_html[\"content\"]\n",
    "    site_name_html = soup.find(\"meta\", property = site_name_property)\n",
    "    site_name = site_name_html[\"content\"]\n",
    "    date_html = soup.find(\"meta\", property = date_property)\n",
    "    date = date_html[\"content\"][0:10]\n",
    "    paragraphs = soup.findAll(\"p\")\n",
    "    articles = ''\n",
    "    for paragraph in paragraphs:\n",
    "        articles += '{} '.format(paragraph)\n",
    "    row = [title, date, articles, site_name]\n",
    "    return row\n",
    "\n",
    "def convert_sites3(root_path, paths, site, title_property, site_name_property, date_property):\n",
    "    csv_name = \"/Users/cn/Desktop/Dissertation/mandm/data/\" + site + \".csv\"\n",
    "    with open(csv_name, 'w', newline='') as csvfile:\n",
    "        csv_file = csv.writer(csvfile, dialect='excel')\n",
    "        csv_file.writerow(headers)\n",
    "        for path in paths:\n",
    "            to_file = root_path + path\n",
    "            #print(to_file)\n",
    "            try:\n",
    "                row = html_to_csv3(to_file, title_property, site_name_property, date_property)\n",
    "            #print(row)\n",
    "                csv_file.writerow(row)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "def convert_all3(all_sites, title_property = \"og:title\",\n",
    "                 site_name_property = \"og:site_name\", date_property = \"og:published_time\"):\n",
    "    for site in all_sites:\n",
    "        try:\n",
    "            root_path = main_dir + site + \"/\"\n",
    "            paths = os.listdir(root_path)\n",
    "            convert_sites2(root_path, paths, site, title_property,\n",
    "                 site_name_property, date_property)\n",
    "            print(site)\n",
    "        except Exception:\n",
    "            print(\"Error: \" + site + \" didn't work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'el_país'\n",
    "root_path = main_dir + site + \"/\"\n",
    "paths = os.listdir(root_path)\n",
    "convert_sites3(root_path, paths, site, \"spc:title\", \"og:site_name\", \"spc:published_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sites4(root_path, html_i):\n",
    "    csv_name = \"/Users/cn/Desktop/Dissertation/mandm/data/\" + site + \".csv\"\n",
    "    with open(csv_name, 'w', newline='') as csvfile:\n",
    "        csv_file = csv.writer(csvfile, dialect='excel')\n",
    "        csv_file.writerow(headers)\n",
    "        for path in paths:\n",
    "            to_file = root_path + path\n",
    "            #print(to_file)\n",
    "            try:\n",
    "                row = html_i\n",
    "            #print(row)\n",
    "                csv_file.writerow(row)\n",
    "            except Exception:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_csv4(file_path):\n",
    "    file = open(file_path, 'r')\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "    title_html = soup.find(\"meta\", property = \"og:title\")\n",
    "    title = title_html[\"content\"]\n",
    "    site_name_html = soup.find(\"meta\", property = \"og:site_name\")\n",
    "    site_name = site_name_html[\"content\"]\n",
    "    date_html = soup.find(\"meta\", name = \"cXenseParse:recs:publishtime\")\n",
    "    date = date_html[\"content\"][0:10]\n",
    "    paragraphs = soup.findAll(\"p\")\n",
    "    articles = ''\n",
    "    for paragraph in paragraphs:\n",
    "        articles += '{} '.format(paragraph)\n",
    "    row = [title, date, articles, site_name]\n",
    "    return row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "find() got multiple values for argument 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m root_path \u001b[39m=\u001b[39m main_dir \u001b[39m+\u001b[39m site \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m paths \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlistdir(root_path)\n\u001b[0;32m----> 4\u001b[0m html_to_csv4(\u001b[39m\"\u001b[39;49m\u001b[39m/Users/cn/Desktop/Dissertation/mandm/data/articles/el_heraldo/el_heraldo_1.html\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m, in \u001b[0;36mhtml_to_csv4\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m site_name_html \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39mmeta\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mproperty\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mog:site_name\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m site_name \u001b[39m=\u001b[39m site_name_html[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m date_html \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39;49mfind(\u001b[39m\"\u001b[39;49m\u001b[39mmeta\u001b[39;49m\u001b[39m\"\u001b[39;49m, name \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mcXenseParse:recs:publishtime\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m date \u001b[39m=\u001b[39m date_html[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m:\u001b[39m10\u001b[39m]\n\u001b[1;32m     10\u001b[0m paragraphs \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfindAll(\u001b[39m\"\u001b[39m\u001b[39mp\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: find() got multiple values for argument 'name'"
     ]
    }
   ],
   "source": [
    "\n",
    "site = 'el_heraldo'\n",
    "root_path = main_dir + site + \"/\"\n",
    "paths = os.listdir(root_path)\n",
    "html_to_csv4(\"/Users/cn/Desktop/Dissertation/mandm/data/articles/el_heraldo/el_heraldo_1.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
