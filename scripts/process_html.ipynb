{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "import multiprocess as mp\n",
    "import re\n",
    "\n",
    "\n",
    "main_dir = \"/Users/cn/Desktop/Dissertation/mandm/data/articles/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_csv(file_path):\n",
    "    file = open(file_path, 'r')\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "    title_html = soup.find(\"meta\", property = \"og:title\")\n",
    "    title = title_html[\"content\"]\n",
    "    site_name_html = soup.find(\"meta\", property=\"og:site_name\")\n",
    "    site_name = site_name_html[\"content\"]\n",
    "    date_html = soup.find(\"meta\", property = \"article:published_time\")\n",
    "    date = date_html[\"content\"][0:10]\n",
    "    paragraphs = soup.findAll(\"p\")\n",
    "    articles = ''\n",
    "    for paragraph in paragraphs:\n",
    "        articles += '{} '.format(paragraph)\n",
    "    \n",
    "    articles = re.sub('<[^<]+?>', '', articles)\n",
    "    row = [title, date, articles, site_name]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "headers = [\"title\", \"date\", \"article\", \"site_name\"]\n",
    "root_path = \"/Users/cn/Desktop/Dissertation/mandm/data/articles/al_dia/\"\n",
    "paths = os.listdir(root_path)\n",
    "\n",
    "\n",
    "with open(\"/Users/cn/Desktop/Dissertation/mandm/data/al_dia.csv\", 'w', newline='') as csvfile:\n",
    "    csv_file = csv.writer(csvfile, dialect='excel')\n",
    "    csv_file.writerow(headers)\n",
    "    for path in paths:\n",
    "        to_file = root_path + path\n",
    "        #print(to_file)\n",
    "        try:\n",
    "            row = html_to_csv(to_file, \"body\")\n",
    "        #print(row)\n",
    "            csv_file.writerow(row)\n",
    "        except Exception:\n",
    "            pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "root_path = \"/Users/cn/Desktop/Dissertation/mandm/data/articles/diario_del_césar/\"\n",
    "paths = os.listdir(root_path)\n",
    "\n",
    "\n",
    "with open(\"/Users/cn/Desktop/Dissertation/mandm/data/diario_del_césar.csv\", 'w', newline='') as csvfile:\n",
    "    csv_file = csv.writer(csvfile, dialect='excel')\n",
    "    csv_file.writerow(headers)\n",
    "    for path in paths:\n",
    "        to_file = root_path + path\n",
    "        #print(to_file)\n",
    "        try:\n",
    "            row = html_to_csv(to_file)\n",
    "        #print(row)\n",
    "            csv_file.writerow(row)\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sites(root_path, paths, site):\n",
    "    csv_name = \"/Users/cn/Desktop/Dissertation/mandm/data/\" + site + \".csv\"\n",
    "    with open(csv_name, 'w', newline='') as csvfile:\n",
    "        csv_file = csv.writer(csvfile, dialect='excel')\n",
    "        csv_file.writerow(headers)\n",
    "        for path in paths:\n",
    "            to_file = root_path + path\n",
    "            #print(to_file)\n",
    "            try:\n",
    "                row = html_to_csv(to_file)\n",
    "            #print(row)\n",
    "                csv_file.writerow(row)\n",
    "            except Exception:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "site = \"diario_del_norte\"\n",
    "root_path = main_dir + site + \"/\"\n",
    "paths = os.listdir(root_path)\n",
    "\n",
    "convert_sites(root_path, paths, site)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "site = \"diario_del_sur\"\n",
    "root_path = main_dir + site + \"/\"\n",
    "paths = os.listdir(root_path)\n",
    "\n",
    "convert_sites(root_path, paths, site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"title\", \"date\", \"article\", \"site_name\"]\n",
    "all_sites = os.listdir(main_dir)\n",
    "all_sites.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_all(all_sites):\n",
    "    for site in all_sites:\n",
    "        try:\n",
    "            root_path = main_dir + site + \"/\"\n",
    "            paths = os.listdir(root_path)\n",
    "            convert_sites(root_path, paths, site)\n",
    "            print(site)\n",
    "        except Exception:\n",
    "            print(\"Error: \" + site + \" didn't work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el_diario\n",
      "el_heraldo\n",
      "vanguardia_liberal\n",
      "vanguardia_liberal_2\n",
      "el_país\n",
      "hoy_diario_de_magdalena_2\n",
      "el_meridiano\n",
      "el_nuevo_liberal\n",
      "elcolombiano\n",
      "el_manduco\n",
      "diario_del_norte\n",
      "la_opinion\n",
      "el_expreso\n",
      "la_nación\n",
      "periodismo_público\n",
      "la_crónica_del_quíndio\n",
      "eje21\n",
      "el_nuevo_dia\n",
      "llano_siete_dias\n",
      "diario_del_césar\n",
      "el_universal_2\n",
      "el_universal\n",
      "the_archipielago_press\n",
      "al_dia\n",
      "diario_del_sur\n",
      "hoy_diario_de_magdalena\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[None]]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = mp.Pool(mp.cpu_count()-2)\n",
    "[pool.map(convert_all, [all_sites])]\n",
    "#convert_all(['el_diario'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'el_país'\n",
    "root_path = main_dir + site + \"/\"\n",
    "paths = os.listdir(root_path)\n",
    "convert_sites2(root_path, paths, site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_csv2(file_path):\n",
    "    file = open(file_path, 'r')\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "    title_html = soup.find(\"meta\", property = \"spc:title\")\n",
    "    title = title_html[\"content\"]\n",
    "    site_name_html = soup.find(\"meta\", property=\"og:site_name\")\n",
    "    site_name = site_name_html[\"content\"]\n",
    "    date_html = soup.find(\"meta\", property = \"spc:published_time\")\n",
    "    date = date_html[\"content\"][0:10]\n",
    "    paragraphs = soup.findAll(\"p\")\n",
    "    articles = ''\n",
    "    for paragraph in paragraphs:\n",
    "        articles += '{} '.format(paragraph)    \n",
    "    \n",
    "    articles = re.sub('<[^<]+?>', '', articles)\n",
    "    row = [title, date, articles, site_name]\n",
    "    return row\n",
    "\n",
    "def convert_sites2(root_path, paths, site):\n",
    "    csv_name = \"/Users/cn/Desktop/Dissertation/mandm/data/\" + site + \".csv\"\n",
    "    with open(csv_name, 'w', newline='') as csvfile:\n",
    "        csv_file = csv.writer(csvfile, dialect='excel')\n",
    "        csv_file.writerow(headers)\n",
    "        for path in paths:\n",
    "            to_file = root_path + path\n",
    "            #print(to_file)\n",
    "            try:\n",
    "                row = html_to_csv2(to_file)\n",
    "            #print(row)\n",
    "                csv_file.writerow(row)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "def convert_all2(all_sites):\n",
    "    for site in all_sites:\n",
    "        try:\n",
    "            root_path = main_dir + site + \"/\"\n",
    "            paths = os.listdir(root_path)\n",
    "            convert_sites2(root_path, paths, site)\n",
    "            print(site)\n",
    "        except Exception:\n",
    "            print(\"Error: \" + site + \" didn't work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"title\", \"date\", \"article\", \"site_name\"]\n",
    "all_sites2 = ['el_heraldo', 'vanguardia_liberal', 'vanguardia_liberal_2',\n",
    "    'el_país', 'el_meridiano', 'el_nuevo_liberal', 'la_opinion', 'el_expreso',\n",
    "    'el_nuevo_dia', 'diario_del_césar', 'la_crónica_del_quíndio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el_heraldo\n",
      "vanguardia_liberal\n",
      "vanguardia_liberal_2\n",
      "el_país\n",
      "el_meridiano\n",
      "el_nuevo_liberal\n",
      "la_opinion\n",
      "el_expreso\n",
      "el_nuevo_dia\n",
      "diario_del_césar\n",
      "la_crónica_del_quíndio\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[None]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = mp.Pool(mp.cpu_count()-2)\n",
    "[pool.map(convert_all, [all_sites2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_csv3(file_path, title_property, site_name_property, date_property):\n",
    "    file = open(file_path, 'r')\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "    title_html = soup.find(\"meta\", property = title_property)\n",
    "    title = title_html[\"content\"]\n",
    "    site_name_html = soup.find(\"meta\", property = site_name_property)\n",
    "    site_name = site_name_html[\"content\"]\n",
    "    date_html = soup.find(\"meta\", property = date_property)\n",
    "    date = date_html[\"content\"][0:10]\n",
    "    paragraphs = soup.findAll(\"p\")\n",
    "    articles = ''\n",
    "    for paragraph in paragraphs:\n",
    "        articles += '{} '.format(paragraph)\n",
    "    \n",
    "    articles = re.sub('<[^<]+?>', '', articles)\n",
    "    row = [title, date, articles, site_name]\n",
    "    return row\n",
    "\n",
    "def convert_sites3(root_path, paths, site, title_property, site_name_property, date_property):\n",
    "    csv_name = \"/Users/cn/Desktop/Dissertation/mandm/data/\" + site + \".csv\"\n",
    "    with open(csv_name, 'w', newline='') as csvfile:\n",
    "        csv_file = csv.writer(csvfile, dialect='excel')\n",
    "        csv_file.writerow(headers)\n",
    "        for path in paths:\n",
    "            to_file = root_path + path\n",
    "            #print(to_file)\n",
    "            try:\n",
    "                row = html_to_csv3(to_file, title_property, site_name_property, date_property)\n",
    "            #print(row)\n",
    "                csv_file.writerow(row)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "def convert_all3(all_sites, title_property = \"og:title\",\n",
    "                 site_name_property = \"og:site_name\", date_property = \"og:published_time\"):\n",
    "    for site in all_sites:\n",
    "        try:\n",
    "            root_path = main_dir + site + \"/\"\n",
    "            paths = os.listdir(root_path)\n",
    "            convert_sites2(root_path, paths, site, title_property,\n",
    "                 site_name_property, date_property)\n",
    "            print(site)\n",
    "        except Exception:\n",
    "            print(\"Error: \" + site + \" didn't work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'el_país'\n",
    "root_path = main_dir + site + \"/\"\n",
    "paths = os.listdir(root_path)\n",
    "convert_sites3(root_path, paths, site, \"spc:title\", \"og:site_name\", \"spc:published_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sites4(root_path, html_i):\n",
    "    csv_name = \"/Users/cn/Desktop/Dissertation/mandm/data/\" + site + \".csv\"\n",
    "    with open(csv_name, 'w', newline='') as csvfile:\n",
    "        csv_file = csv.writer(csvfile, dialect='excel')\n",
    "        csv_file.writerow(headers)\n",
    "        for path in paths:\n",
    "            to_file = root_path + path\n",
    "            #print(to_file)\n",
    "            try:\n",
    "                row = html_i(to_file)\n",
    "            #print(row)\n",
    "                csv_file.writerow(row)\n",
    "            except Exception:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_csv4(file_path):\n",
    "    file = open(file_path, 'r')\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "    title_html = soup.find(\"meta\", property = \"og:title\")\n",
    "    title = title_html[\"content\"]\n",
    "    site_name_html = soup.find(\"meta\", property = \"og:site_name\")\n",
    "    site_name = site_name_html[\"content\"]\n",
    "    date_html = soup.select('meta[name=\"cXenseParse:recs:publishtime\"]')\n",
    "    date = str(date_html)[16:26]#re.search(\"content\", date_html)\n",
    "    paragraphs = soup.findAll(\"p\")\n",
    "    articles = ''\n",
    "    for paragraph in paragraphs:\n",
    "        articles += '{} '.format(paragraph)\n",
    "\n",
    "    articles = re.sub('<[^<]+?>', '', articles)\n",
    "    row = [title, date, articles, site_name]\n",
    "    return row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'el_heraldo'\n",
    "root_path = main_dir + site + \"/\"\n",
    "paths = os.listdir(root_path)\n",
    "convert_sites4(root_path, html_to_csv4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    " def html_to_csv4(file_path):\n",
    "    file = open(file_path, 'r')\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "    title_html = soup.find(\"meta\", property = \"og:title\")\n",
    "    title = title_html[\"content\"]\n",
    "    site_name_html = soup.find(\"meta\", property = \"og:site_name\")\n",
    "    site_name = site_name_html[\"content\"]\n",
    "    date_html = soup.select('meta[name=\"cXenseParse:recs:publishtime\"]')\n",
    "    date = str(date_html)[16:26]#re.search(\"content\", date_html)\n",
    "    paragraphs = soup.findAll(\"p\")\n",
    "    articles = ''\n",
    "    for paragraph in paragraphs:\n",
    "        articles += '{} '.format(paragraph)\n",
    "\n",
    "    articles = re.sub('<[^<]+?>', '', articles)\n",
    "    row = [title, date, articles, site_name]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = mp.Pool(mp.cpu_count()-2)\n",
    "\n",
    "site = 'vanguardia_liberal'\n",
    "root_path = main_dir + site + \"/\"\n",
    "paths = os.listdir(root_path)\n",
    "convert_sites4(root_path, html_to_csv4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'vanguardia_liberal_2'\n",
    "root_path = main_dir + site + \"/\"\n",
    "paths = os.listdir(root_path)\n",
    "convert_sites4(root_path, html_to_csv4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL,'es_ES.UTF-8')\n",
    "\n",
    "def html_to_csv5(file_path):\n",
    "    file = open(file_path, 'r')\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "    title = soup.title.text\n",
    "   # title = title_html[\"content\"]\n",
    "    site_name = soup.h2.a.text\n",
    "    date_html = soup.find(class_ = \"date\").find(\"li\").text\n",
    "    date = re.sub(r'^.*?,\\s', '', date_html)\n",
    "    date = datetime.strptime(date, \"%B %d, %Y\").strftime(\"%Y-%m-%d\")\n",
    "    paragraphs = soup.findAll(\"p\")\n",
    "    articles = ''\n",
    "    for paragraph in paragraphs:\n",
    "        articles += '{} '.format(paragraph)\n",
    "    \n",
    "    articles = re.sub('<[^<]+?>', '', articles)\n",
    "    row = [title, date, articles, site_name]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'el_nuevo_liberal'\n",
    "root_path = main_dir + site + \"/\"\n",
    "paths = os.listdir(root_path)\n",
    "convert_sites4(root_path, html_to_csv5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    " def html_to_csv6(file_path):\n",
    "    file = open(file_path, 'r')\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "    title_html = soup.find(\"meta\", property = \"og:title\")\n",
    "    title = title_html[\"content\"]\n",
    "    site_name_html = soup.find(\"meta\", property = \"og:site_name\")\n",
    "    site_name = site_name_html[\"content\"]\n",
    "    date_html = soup.select('meta[name=\"cXenseParse:recs:publishtime\"]')\n",
    "    date = soup.time.text\n",
    "    date = datetime.strptime(date, \"%a, %d/%m/%Y - %H:%M\").strftime(\"%Y-%m-%d\")\n",
    "    paragraphs = soup.findAll(\"p\")\n",
    "    articles = ''\n",
    "    for paragraph in paragraphs:\n",
    "        articles += '{} '.format(paragraph)\n",
    "    articles = re.sub('<[^<]+?>', '', articles)\n",
    "    row = [title, date, articles, site_name]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'el_nuevo_dia'\n",
    "root_path = main_dir + site + \"/\"\n",
    "paths = os.listdir(root_path)\n",
    "convert_sites4(root_path, html_to_csv6)\n",
    "#html_to_csv6(\"/Users/cn/Desktop/Dissertation/mandm/data/articles/el_nuevo_dia/el_nuevo_dia_1.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    " def html_to_csv7(file_path):\n",
    "    file = open(file_path, 'r')\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "    title_html = soup.find(\"meta\", property = \"og:title\")\n",
    "    title = title_html[\"content\"]\n",
    "    site_name_html = soup.find(\"meta\", property = \"og:site_name\")\n",
    "    site_name = site_name_html[\"content\"]\n",
    "    date = soup.script.text\n",
    "    date = re.search(\"datePublished.*\", date)\n",
    "    date = re.search(\"[0-9]+/[0-9]+/[0-9]+\", str(date))\n",
    "    date = datetime.strptime(date[0], \"%d/%m/%Y\").strftime(\"%Y-%m-%d\")\n",
    "    paragraphs = soup.findAll(\"p\")\n",
    "    articles = ''\n",
    "    for paragraph in paragraphs:\n",
    "        articles += '{} '.format(paragraph)\n",
    "    articles = re.sub('<[^<]+?>', '', articles)\n",
    "    row = [title, date, articles, site_name]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'la_opinion'\n",
    "root_path = main_dir + site + \"/\"\n",
    "paths = os.listdir(root_path)\n",
    "convert_sites4(root_path, html_to_csv7)\n",
    "#html_to_csv7(\"/Users/cn/Desktop/Dissertation/mandm/data/articles/la_opinion/la_opinion_1.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    " def html_to_csv8(file_path):\n",
    "    file = open(file_path, 'r')\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "    title = soup.title.text\n",
    "    site_name = \"El Expreso\"\n",
    "    date = soup.findAll(\"script\", type=\"application/ld+json\")[1]\n",
    "    date = re.search(\"datePublished.*\", str(date))\n",
    "    date = re.search(\"[0-9]+-[0-9]+-[0-9]+\", str(date))\n",
    "    date = datetime.strptime(date[0], \"%Y-%m-%d\").strftime(\"%Y-%m-%d\")\n",
    "    paragraphs = soup.findAll(\"p\")\n",
    "    articles = ''\n",
    "    for paragraph in paragraphs:\n",
    "        articles += '{} '.format(paragraph)\n",
    "\n",
    "    articles = re.sub('<[^<]+?>', '', articles)\n",
    "    row = [title, date, articles, site_name]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'el_expreso'\n",
    "root_path = main_dir + site + \"/\"\n",
    "paths = os.listdir(root_path)\n",
    "convert_sites4(root_path, html_to_csv8)\n",
    "#html_to_csv8(\"/Users/cn/Desktop/Dissertation/mandm/data/articles/el_expreso/el_expreso_11.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    " def html_to_csv9(file_path):\n",
    "    file = open(file_path, 'r')\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "    title_html = soup.find(\"meta\", property = \"og:title\")\n",
    "    title = title_html[\"content\"]\n",
    "    site_name_html = soup.find(\"meta\", property = \"og:site_name\")\n",
    "    site_name = site_name_html[\"content\"]\n",
    "    date = soup.findAll(\"script\", type=\"application/ld+json\")[0]\n",
    "    date = re.search(\"datePublished.*\", str(date))\n",
    "    date = re.search(\"[0-9]+-[0-9]+-[0-9]+\", str(date))\n",
    "    date = datetime.strptime(date[0], \"%Y-%m-%d\").strftime(\"%Y-%m-%d\")\n",
    "    paragraphs = soup.findAll(\"p\")\n",
    "    articles = ''\n",
    "    for paragraph in paragraphs:\n",
    "        articles += '{} '.format(paragraph)\n",
    "    articles = re.sub('<[^<]+?>', '', articles)\n",
    "    row = [title, date, articles, site_name]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'la_crónica_del_quíndio'\n",
    "root_path = main_dir + site + \"/\"\n",
    "paths = os.listdir(root_path)\n",
    "convert_sites4(root_path, html_to_csv9)\n",
    "#html_to_csv9(\"/Users/cn/Desktop/Dissertation/mandm/data/articles/la_crónica_del_quíndio/la_crónica_del_quíndio_1.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_csv10(file_path):\n",
    "    file = open(file_path, 'r')\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "    title_html = soup.find(\"meta\", property = \"og:title\")\n",
    "    title = title_html[\"content\"]\n",
    "    site_name_html = soup.find(\"meta\", property = \"og:site_name\")\n",
    "    site_name = site_name_html[\"content\"]\n",
    "    date = soup.findAll(\"script\", type=\"application/ld+json\")[2]\n",
    "    date = re.search(\"datePublished.*\", str(date))\n",
    "    date = re.search(\"[0-9]+-[0-9]+-[0-9]+\", str(date))\n",
    "    date = datetime.strptime(date[0], \"%Y-%m-%d\").strftime(\"%Y-%m-%d\")\n",
    "    paragraphs = soup.findAll(\"p\")\n",
    "    articles = ''\n",
    "    for paragraph in paragraphs:\n",
    "        articles += '{} '.format(paragraph)\n",
    "\n",
    "    articles = re.sub('<[^<]+?>', '', articles)\n",
    "    row = [title, date, articles, site_name]\n",
    "    return row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'diario_del_césar'\n",
    "root_path = main_dir + site + \"/\"\n",
    "paths = os.listdir(root_path)\n",
    "convert_sites4(root_path, html_to_csv10)\n",
    "#html_to_csv10(\"/Users/cn/Desktop/Dissertation/mandm/data/articles/diario_del_césar/diario_del_césar_1.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_csv11(file_path):\n",
    "    file = open(file_path, 'r')\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "    title_html = soup.find(\"meta\", property = \"og:title\")\n",
    "    title = title_html[\"content\"]\n",
    "    site_name_html = soup.find(\"meta\", property = \"og:site_name\")\n",
    "    site_name = site_name_html[\"content\"]\n",
    "    date = soup.div(class_ = \"fecha\")[1].text\n",
    "    #date = re.search(\"datePublished.*\", str(date))\n",
    "    #date = re.search(\"[0-9]+-[0-9]+-[0-9]+\", str(date))\n",
    "    date = datetime.strptime(date, \"%d de %B de %Y\").strftime(\"%Y-%m-%d\")\n",
    "    paragraphs = soup.findAll(\"p\")\n",
    "    articles = ''\n",
    "    for paragraph in paragraphs:\n",
    "        articles += '{} '.format(paragraph)\n",
    "\n",
    "    articles = re.sub('<[^<]+?>', '', articles)\n",
    "    row = [title, date, articles, site_name]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'eje21'\n",
    "root_path = main_dir + site + \"/\"\n",
    "paths = os.listdir(root_path)\n",
    "convert_sites4(root_path, html_to_csv11)\n",
    "#html_to_csv11(\"/Users/cn/Desktop/Dissertation/mandm/data/articles/eje21/eje21_56.html\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sites that didn't work with the default functions above:\n",
    "\n",
    "1. ~~diario del cesar~~\n",
    "3. ~~la cronica del quindio~~\n",
    "4. ~~el expreso~~\n",
    "5. ~~la opinion~~\n",
    "7. ~~el nuevo dia~~\n",
    "8. ~~el nuevo liberal~~\n",
    "9. ~~vanguardia liberal~~ \n",
    "10. ~~vanguardia liberal2~~\n",
    "11. ~~el pais~~\n",
    "12. ~~el heraldo~~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
